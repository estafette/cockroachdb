apiVersion: v1
kind: Namespace
metadata:
  name: ${NAMESPACE}
---
apiVersion: v1
kind: Service
metadata:
  # This service is meant to be used by clients of the database. It exposes a ClusterIP that will
  # automatically load balance connections to the different database pods.
  name: ${APP_NAME}-public
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}
    team: ${TEAM_NAME}
spec:
  ports:
  # The main port, served by gRPC, serves Postgres-flavor SQL, internode
  # traffic and the cli.
  - port: 26257
    targetPort: 26257
    name: grpc
  # The secondary port serves the UI as well as health and debug endpoints.
  - port: 8080
    targetPort: 8080
    name: http
  selector:
    app: ${APP_NAME}
---
apiVersion: v1
kind: Service
metadata:
  # This service only exists to create DNS entries for each pod in the stateful
  # set such that they can resolve each other's IP addresses. It does not
  # create a load-balanced ClusterIP and should not be used directly by clients
  # in most circumstances.
  name: ${APP_NAME}
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}
    team: ${TEAM_NAME}
  annotations:
    # This is needed to make the peer-finder work properly and to help avoid
    # edge cases where instance 0 comes up after losing its data and needs to
    # decide whether it should create a new cluster or try to join an existing
    # one. If it creates a new cluster when it should have joined an existing
    # one, we'd end up with two separate clusters listening at the same service
    # endpoint, which would be very bad.
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  ports:
  - port: 26257
    targetPort: 26257
    name: grpc
  - port: 8080
    targetPort: 8080
    name: http
  clusterIP: None
  selector:
    app: ${APP_NAME}
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: ${APP_NAME}-budget
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}
    team: ${TEAM_NAME}
spec:
  selector:
    matchLabels:
      app: ${APP_NAME}
  maxUnavailable: 1
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: ${APP_NAME}
spec:
  serviceName: "${APP_NAME}"
  replicas: ${REPLICAS}
  updateStrategy:
    type: "RollingUpdate"
  selector:
    matchLabels:
      app: ${APP_NAME}
  template:
    metadata:
      labels:
        app: ${APP_NAME}
        team: ${TEAM_NAME}
      annotations:
        # Enable automatic monitoring of all instances when Prometheus is running in the cluster.
        prometheus.io/scrape: "true"
        prometheus.io/path: "_status/vars"
        prometheus.io/port: "8080"
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - ${APP_NAME}
            topologyKey: kubernetes.io/hostname
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 10
            preference:
              matchExpressions:
              - key: cloud.google.com/gke-preemptible
                operator: In
                values:
                - "true"
      containers:
      - name: ${APP_NAME}
        image: cockroachdb/cockroach:${COCKROACHDB_VERSION}
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            cpu: ${CPU_REQUEST}
            memory: ${MEMORY_REQUEST}
          limits:
            cpu: ${CPU_LIMIT}
            memory: ${MEMORY_LIMIT}
        env:
        - name: "COCKROACH_SKIP_ENABLING_DIAGNOSTIC_REPORTING"
          value: "true"
        ports:
        - containerPort: 26257
          name: grpc
        - containerPort: 8080
          name: http
        volumeMounts:
        - name: ${APP_NAME}-datadir
          mountPath: /cockroach/cockroach-data
        command:
          - "/bin/bash"
          - "-ecx"
          # The use of qualified `hostname -f` is crucial:
          # Other nodes aren't able to look up the unqualified hostname.
          - "exec /cockroach/cockroach start --logtostderr --insecure --advertise-host $(hostname -f) --http-host 0.0.0.0 --join ${APP_NAME}-0.${APP_NAME},${APP_NAME}-1.${APP_NAME},${APP_NAME}-2.${APP_NAME} --cache=.35 --max-sql-memory=.35 --locality ${LOCALITY}"
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 30
          timeoutSeconds: 5
      # No pre-stop hook is required, a SIGTERM plus some time is all that's
      # needed for graceful shutdown of a node.
      terminationGracePeriodSeconds: 60
      volumes:
      - name: ${APP_NAME}-datadir
        persistentVolumeClaim:
          claimName: ${APP_NAME}-datadir
  volumeClaimTemplates:
  - metadata:
      name: ${APP_NAME}-datadir
      namespace: ${NAMESPACE}
      annotations:
        volume.beta.kubernetes.io/storage-class: ${STORAGE_CLASS}
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: ${PVC_SIZE}
---
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: ${APP_NAME}
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}
    team: ${TEAM_NAME}
  annotations:
    kubernetes.io/ingress.class: "gce"
    kubernetes.io/ingress.allow-http: "false"
    estafette.io/cloudflare-dns: "true"
    estafette.io/cloudflare-proxy: "true"
    estafette.io/cloudflare-hostnames: "${HOSTNAMES}"
spec:
  tls:
  - hosts:
    - ${HOSTNAMES}
    secretName: ${APP_NAME}-letsencrypt-certificate
  rules:
  - host: ${HOSTNAMES}
    http:
      paths:
      - path: /*
        backend:
          serviceName: ${APP_NAME}-admin
          servicePort: http
---
apiVersion: v1
kind: Service
metadata:
  name: ${APP_NAME}-admin
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}-admin
    team: ${TEAM_NAME}
  annotations:
    prometheus.io/probe: "true"
    prometheus.io/probe-path: "/health"
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    targetPort: http
    protocol: TCP
  - name: https
    port: 443
    targetPort: https
    protocol: TCP
  selector:
    app: ${APP_NAME}-admin
---
apiVersion: v1
kind: Secret
metadata:
  name: ${APP_NAME}-letsencrypt-certificate
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}
    team: ${TEAM_NAME}
  annotations:
    estafette.io/letsencrypt-certificate: "true"
    estafette.io/letsencrypt-certificate-hostnames: "${HOSTNAMES}"
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: ${APP_NAME}-admin
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}-admin
    team: ${TEAM_NAME}
spec:
  selector:
    matchLabels:
      app: ${APP_NAME}-admin
  minAvailable: 2
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: ${APP_NAME}-admin
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}-admin
    team: ${TEAM_NAME}
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: ${APP_NAME}-admin
  template:
    metadata:
      labels:
        app: ${APP_NAME}-admin
        team: ${TEAM_NAME}
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9101"
    spec:
      affinity:
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: app
                  operator: In
                  values:
                  - ${APP_NAME}-admin
              topologyKey: kubernetes.io/hostname
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 10
            preference:
              matchExpressions:
              - key: cloud.google.com/gke-preemptible
                operator: In
                values:
                - "true"
      containers:
      - name: ${APP_NAME}-openresty
        image: estafette/openresty-sidecar:1.13.6.2-alpine
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            cpu: 50m
            memory: 50Mi
          requests:
            cpu: 10m
            memory: 10Mi
        ports:
        - name: http
          containerPort: 80
        - name: https
          containerPort: 443
        - name: nginx-liveness
          containerPort: 82
        - name: nginx-readiness
          containerPort: 81
        - name: nginx-prom
          containerPort: 9101
        env:
        - name: "OFFLOAD_TO_HOST"
          value: "${APP_NAME}-public"
        - name: "OFFLOAD_TO_PORT"
          value: "8080"
        - name: "SERVICE_NAME"
          value: "${APP_NAME}-admin"
        - name: "NAMESPACE"
          value: "${NAMESPACE}"
        - name: "HEALT_CHECK_PATH"
          value: "/health"
        - name: "ENFORCE_HTTPS"
          value: "false"
        volumeMounts:
        - name: ssl-certificate
          mountPath: /etc/ssl/private
        livenessProbe:
          httpGet:
            path: /liveness
            port: nginx-liveness
          initialDelaySeconds: 15
        readinessProbe:
          httpGet:
            path: /health
            port: http
      volumes:
      - name: ssl-certificate
        secret:
          secretName: ${APP_NAME}-letsencrypt-certificate
# ---
# apiVersion: batch/v1
# kind: Job
# metadata:
#   name: ${APP_NAME}-cluster-init
#   namespace: ${NAMESPACE}
#   labels:
#     app: ${APP_NAME}
#     team: ${TEAM_NAME}
# spec:
#   template:
#     spec:
#       containers:
#       - name: cluster-init
#         image: cockroachdb/cockroach:${COCKROACHDB_VERSION}
#         imagePullPolicy: IfNotPresent
#         command:
#           - "/cockroach/cockroach"
#           - "init"
#           - "--insecure"
#           - "--host=${APP_NAME}-0.${APP_NAME}"
#       restartPolicy: OnFailure