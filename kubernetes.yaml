apiVersion: v1
kind: Namespace
metadata:
  name: ${NAMESPACE}
---
apiVersion: v1
kind: Service
metadata:
  # This service is meant to be used by clients of the database. It exposes a ClusterIP that will
  # automatically load balance connections to the different database pods.
  name: ${APP_NAME}-public
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}
    team: ${TEAM_NAME}
spec:
  ports:
  # The main port, served by gRPC, serves Postgres-flavor SQL, internode
  # traffic and the cli.
  - port: 26257
    targetPort: 26257
    name: grpc
  # The secondary port serves the UI as well as health and debug endpoints.
  - port: 8080
    targetPort: 8080
    name: http
  selector:
    app: ${APP_NAME}
---
apiVersion: v1
kind: Service
metadata:
  # This service only exists to create DNS entries for each pod in the stateful
  # set such that they can resolve each other's IP addresses. It does not
  # create a load-balanced ClusterIP and should not be used directly by clients
  # in most circumstances.
  name: ${APP_NAME}
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}
    team: ${TEAM_NAME}
  annotations:
    # This is needed to make the peer-finder work properly and to help avoid
    # edge cases where instance 0 comes up after losing its data and needs to
    # decide whether it should create a new cluster or try to join an existing
    # one. If it creates a new cluster when it should have joined an existing
    # one, we'd end up with two separate clusters listening at the same service
    # endpoint, which would be very bad.
    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
spec:
  ports:
  - port: 26257
    targetPort: 26257
    name: grpc
  - port: 8080
    targetPort: 8080
    name: http
  clusterIP: None
  selector:
    app: ${APP_NAME}
---
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: ${APP_NAME}-budget
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}
    team: ${TEAM_NAME}
spec:
  selector:
    matchLabels:
      app: ${APP_NAME}
  minAvailable: 67%
---
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: ${APP_NAME}
spec:
  serviceName: "${APP_NAME}"
  replicas: ${REPLICAS}
  updateStrategy:
    type: "RollingUpdate"
  template:
    metadata:
      labels:
        app: ${APP_NAME}
      annotations:
        # Enable automatic monitoring of all instances when Prometheus is running in the cluster.
        prometheus.io/scrape: "true"
        prometheus.io/path: "_status/vars"
        prometheus.io/port: "8080"
    spec:
      # Init containers are run only once in the lifetime of a pod, before
      # it's started up for the first time. It has to exit successfully
      # before the pod's main containers are allowed to start.
      # This particular init container does a DNS lookup for other pods in
      # the set to help determine whether or not a cluster already exists.
      # If any other pods exist, it creates a file in the cockroach-data
      # directory to pass that information along to the primary container that
      # has to decide what command-line flags to use when starting CockroachDB.
      # This only matters when a pod's persistent volume is empty - if it has
      # data from a previous execution, that data will always be used.
      initContainers:
      - name: bootstrap
        image: cockroachdb/cockroach-k8s-init:0.1
        imagePullPolicy: IfNotPresent
        args: ["-on-start=/on-start.sh", "-service=${APP_NAME}"]
        env:
        - name: "POD_NAMESPACE"
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        volumeMounts:
        - mountPath: /cockroach/cockroach-data
          name: ${APP_NAME}-datadir
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: app
                operator: In
                values:
                - ${APP_NAME}
            topologyKey: kubernetes.io/hostname
      containers:
      - name: ${APP_NAME}
        # Runs the master branch. Not recommended for production, but since
        # CockroachDB is in Beta, you don't want to run it in production
        # anyway. See
        # https://hub.docker.com/r/cockroachdb/cockroach/tags/
        # if you prefer to run a beta release.
        image: cockroachdb/cockroach:${COCKROACHDB_VERSION}
        imagePullPolicy: IfNotPresent
        resources:
          requests:
            cpu: ${CPU_REQUEST}
            memory: ${MEMORY_REQUEST}
          limits:
            cpu: ${CPU_LIMIT}
            memory: ${MEMORY_LIMIT}
        ports:
        - containerPort: 26257
          name: grpc
        - containerPort: 8080
          name: http
        volumeMounts:
        - name: ${APP_NAME}-datadir
          mountPath: /cockroach/cockroach-data
        command:
          - "/bin/bash"
          - "-ecx"
          - |
            # The use of qualified `hostname -f` is crucial:
            # Other nodes aren't able to look up the unqualified hostname.
            CRARGS=("start" "--logtostderr" "--insecure" "--host" "$(hostname -f)" "--http-host" "0.0.0.0")
            # We only want to initialize a new cluster (by omitting the join flag)
            # if we're sure that we're the first node (i.e. index 0) and that
            # there aren't any other nodes running as part of the cluster that
            # this is supposed to be a part of (which indicates that a cluster
            # already exists and we should make sure not to create a new one).
            # It's fine to run without --join on a restart if there aren't any
            # other nodes.
            if [ ! "$(hostname)" == "${APP_NAME}-0" ] || \
               [ -e "/cockroach/cockroach-data/cluster_exists_marker" ]
            then
              # We don't join cockroachdb in order to avoid a node attempting
              # to join itself, which currently doesn't work
              # (https://github.com/cockroachdb/cockroach/issues/9625).
              CRARGS+=("--join" "${APP_NAME}-public")
            fi
            exec /cockroach/cockroach ${CRARGS[*]}
        livenessProbe:
          httpGet:
            path: /health
            port: http
          initialDelaySeconds: 30
      # No pre-stop hook is required, a SIGTERM plus some time is all that's
      # needed for graceful shutdown of a node.
      terminationGracePeriodSeconds: 60
      volumes:
      - name: ${APP_NAME}-datadir
        persistentVolumeClaim:
          claimName: ${APP_NAME}-datadir
  volumeClaimTemplates:
  - metadata:
      name: ${APP_NAME}-datadir
      namespace: ${NAMESPACE}
      annotations:
        volume.beta.kubernetes.io/storage-class: ${STORAGE_CLASS}
    spec:
      accessModes:
        - "ReadWriteOnce"
      resources:
        requests:
          storage: ${PVC_SIZE}
---
apiVersion: v1
kind: Service
metadata:
  name: ${APP_NAME}-admin
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}-admin
    team: ${TEAM_NAME}
  annotations:
    estafette.io/cloudflare-dns: "${CLOUDFLARE_CREATE_DNS_RECORD}"
    estafette.io/cloudflare-proxy: "${CLOUDFLARE_ENABLE_PROXY}"
    estafette.io/cloudflare-hostnames: "${HOSTNAMES}"
spec:
  type: LoadBalancer
  selector:
    app: ${APP_NAME}-admin
  ports:
  - name: http
    port: 80
    targetPort: http
    protocol: TCP
  - name: https
    port: 443
    targetPort: https
    protocol: TCP
---
apiVersion: v1
kind: Secret
metadata:
  name: ${APP_NAME}-letsencrypt-certificate
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}
    team: ${TEAM_NAME}
  annotations:
    estafette.io/letsencrypt-certificate: "${LETSENCRYPT_CREATE_CERTIFICATE}"
    estafette.io/letsencrypt-certificate-hostnames: "${HOSTNAMES}"
---
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: ${APP_NAME}-admin
  namespace: ${NAMESPACE}
  labels:
    app: ${APP_NAME}-admin
    team: ${TEAM_NAME}
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: ${APP_NAME}-admin
  template:
    metadata:
      labels:
        app: ${APP_NAME}-admin
        team: ${TEAM_NAME}
        version: ${VERSION}
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9101"
    spec:
      containers:
      - name: ${APP_NAME}-nginx
        image: estafette/nginx-sidecar:1.12.1
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            cpu: 50m
            memory: 50Mi
          requests:
            cpu: 10m
            memory: 10Mi
        ports:
        - name: http
          containerPort: 80
        - name: https
          containerPort: 443
        - name: nginx-liveness
          containerPort: 82
        - name: nginx-readiness
          containerPort: 81
        - name: nginx-prom
          containerPort: 9101
        env:
        - name: "OFFLOAD_TO_HOST"
          value: "${APP_NAME}-public"
        - name: "OFFLOAD_TO_PORT"
          value: "8080"
        - name: "SERVICE_NAME"
          value: "${APP_NAME}-admin"
        - name: "NAMESPACE"
          value: "${NAMESPACE}"
        - name: "HEALT_CHECK_PATH"
          value: "/health"
        - name: "ALLOW_CIDRS"
          value: "${ALLOW_CIDRS}"
        volumeMounts:
        - name: ssl-certificate
          mountPath: /etc/ssl/private
        livenessProbe:
          httpGet:
            path: /liveness
            port: nginx-liveness
          initialDelaySeconds: 15
        readinessProbe:
          httpGet:
            path: /readiness
            port: nginx-readiness
      volumes:
      - name: ssl-certificate
        secret:
          secretName: ${APP_NAME}-letsencrypt-certificate